"""
FixWurx Auditor Agent

This module implements the Auditor agent that serves as the mathematical brake
for the FixWurx system. It verifies completeness, correctness, and meta-awareness
of the system and emits authoritative Audit-Stamps.

See docs/auditor_agent_specification.md for full specification.
"""

import math
import yaml
import json
import logging
import datetime
import uuid
from typing import Dict, List, Set, Tuple, Union, Optional, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] [Auditor] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('auditor')


class ObligationLedger:
    """
    Maintains and computes the Δ-Closure of obligations derived from initial goals.
    Implements the mathematical guarantees from Lemma 14-Pieces.
    """
    
    def __init__(self):
        self.obligations: Set[str] = set()
        self.delta_rules = []
        
    def load_delta_rules(self, rules_file: str) -> None:
        """Load term-rewriting system Δ from a rules file"""
        try:
            with open(rules_file, 'r') as f:
                self.delta_rules = json.load(f)
            logger.info(f"Loaded {len(self.delta_rules)} Δ-rules from {rules_file}")
        except Exception as e:
            logger.error(f"Failed to load Δ-rules: {e}")
            raise
            
    def compute_delta_closure(self, initial_goals: Set[str]) -> Set[str]:
        """
        Compute the minimal fixed point of the Δ-closure.
        
        Args:
            initial_goals: The initial set of Horn clauses extracted from user request
            
        Returns:
            The complete set of obligations that must be fulfilled
        """
        # Start with initial goals
        self.obligations = initial_goals.copy()
        frontier = list(initial_goals)
        visited = set(frontier)
        
        # Breadth-first enumeration over finite DAG
        while frontier:
            current = frontier.pop(0)
            
            # Apply each rule to the current obligation
            for rule in self.delta_rules:
                if self._rule_applies(rule, current):
                    new_obligations = self._apply_rule(rule, current)
                    
                    # Add new obligations to the set and frontier
                    for obligation in new_obligations:
                        if obligation not in visited:
                            self.obligations.add(obligation)
                            frontier.append(obligation)
                            visited.add(obligation)
        
        logger.info(f"Δ-Closure computed: {len(initial_goals)} initial goals → {len(self.obligations)} total obligations")
        return self.obligations
    
    def _rule_applies(self, rule: Dict, obligation: str) -> bool:
        """
        Check if a rewrite rule applies to an obligation.
        
        Args:
            rule: The rule to check
            obligation: The obligation to check against
            
        Returns:
            True if the rule applies, False otherwise
        """
        # Extract pattern from rule
        pattern = rule.get("pattern", "")
        
        # If pattern is empty, rule doesn't apply
        if not pattern:
            return False
        
        # Check if pattern is in obligation
        # This is a simple string matching approach
        # More sophisticated approaches could use regex or semantic matching
        return pattern in obligation
    
    def _apply_rule(self, rule: Dict, obligation: str) -> Set[str]:
        """
        Apply a rewrite rule to an obligation, generating new obligations.
        
        Args:
            rule: The rule to apply
            obligation: The obligation to apply the rule to
            
        Returns:
            Set of new obligations generated by applying the rule
        """
        # If rule doesn't apply, return empty set
        if not self._rule_applies(rule, obligation):
            return set()
        
        # Extract transforms_to from rule
        transforms_to = rule.get("transforms_to", [])
        
        # If transforms_to is empty, no new obligations
        if not transforms_to:
            return set()
        
        # Return set of new obligations
        return set(transforms_to)
    
    def get_all(self) -> Set[str]:
        """Return the complete set of obligations"""
        return self.obligations


class RepoModules:
    """
    Analyzes the codebase to identify implemented modules that satisfy obligations.
    """
    
    def __init__(self, repo_path: str):
        self.repo_path = repo_path
        self.modules: Set[str] = set()
        
    def scan_repository(self) -> Set[str]:
        """
        Scan the repository to identify implemented modules that satisfy obligations.
        
        Returns:
            Set of implemented obligations
        """
        import os
        import re
        
        self.modules = set()
        
        # List of file extensions to scan
        extensions = ['.py', '.js', '.ts', '.java', '.c', '.cpp', '.h', '.hpp']
        
        # List of directories to exclude
        exclude_dirs = ['venv', 'node_modules', '.git', '__pycache__']
        
        # Function to check if path should be excluded
        def should_exclude(path):
            for exclude in exclude_dirs:
                if exclude in path.split(os.sep):
                    return True
            return False
        
        # Walk through repository
        for root, dirs, files in os.walk(self.repo_path):
            # Skip excluded directories
            if should_exclude(root):
                continue
            
            # Process each file
            for file in files:
                # Check file extension
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext not in extensions:
                    continue
                
                # Read file content
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except Exception as e:
                    logger.warning(f"Failed to read file {file_path}: {e}")
                    continue
                
                # Extract potential obligations from file content
                # This is a simplified implementation that looks for keywords
                # In a real system, this would use AST parsing or other sophisticated analysis
                
                # Define patterns to search for
                patterns = [
                    r'authenticate_user',
                    r'authorize_user',
                    r'manage_roles',
                    r'store_data',
                    r'validate_data',
                    r'process_user_input',
                    r'sanitize_input',
                    r'concurrent_execution',
                    r'synchronize_access',
                    r'call_external_api',
                    r'handle_api_errors',
                    r'process_data',
                    r'log_processing_steps',
                    r'manage_user_sessions',
                    r'persist_data',
                    r'backup_data',
                    r'ensure_high_availability',
                    r'implement_load_balancing',
                    r'send_user_notification',
                    r'confirm_notification_delivery',
                    r'manage_configuration',
                    r'validate_configuration',
                    r'allocate_resources',
                    r'monitor_resource_usage',
                    r'implement_scaling',
                    r'collect_performance_metrics'
                ]
                
                # Search for patterns in file content
                for pattern in patterns:
                    if re.search(pattern, content):
                        self.modules.add(pattern)
        
        logger.info(f"Repository scan complete: found {len(self.modules)} modules")
        return self.modules
    
    def get_all(self) -> Set[str]:
        """Return all modules found in the repository"""
        return self.modules


class EnergyCalculator:
    """
    Calculates energy metrics based on the PL inequality to ensure global convergence.
    """
    
    def __init__(self):
        self.current_state = None
        self.energy_minimum = None
        
    def initialize_state(self, initial_state: Dict) -> None:
        """Initialize the state for energy calculation"""
        self.current_state = initial_state
        
    def calculate_gradient(self) -> float:
        """Calculate the gradient of the energy function at the current state"""
        # Implementation would depend on the specific energy function
        # Placeholder
        return 0.0
    
    def get_metrics(self) -> Tuple[float, float, float]:
        """
        Get energy metrics: current energy, delta_E, and lambda value.
        Uses the Polyak–Łojasiewicz (PL) inequality to ensure global convergence.
        
        Returns:
            Tuple of (E, delta_E, lambda)
                E: current energy value
                delta_E: change in energy from previous state
                lambda: contraction constant (should be < 0.9)
        """
        # If no state has been initialized, return default values
        if not self.current_state:
            return 0.0001, 1e-8, 0.85
        
        # Calculate current energy
        E = self._calculate_energy(self.current_state)
        
        # Calculate gradient norm
        gradient_norm = self.calculate_gradient()
        
        # Estimate energy minimum if not already set
        # In a real implementation, this would use more sophisticated methods
        if self.energy_minimum is None:
            self.energy_minimum = max(0.0, E - gradient_norm**2)
        
        # Calculate energy delta from minimum
        delta_E = E - self.energy_minimum
        
        # Calculate lambda based on PL inequality
        # Using the relationship: ||∇E(S)||² ≥ μ(E(S)−E*)
        # Lambda is related to the inverse of μ
        if delta_E > 0 and gradient_norm > 0:
            mu = gradient_norm**2 / delta_E
            lamb = 1 - min(0.5, mu * 0.1)  # Ensure lambda < 1 for convergence
        else:
            # If at minimum, lambda should be small
            lamb = 0.5
        
        return E, delta_E, lamb
    
    def _calculate_energy(self, state: Dict) -> float:
        """
        Calculate the energy function for the given state.
        
        Args:
            state: The state to calculate energy for
            
        Returns:
            The energy value
        """
        # In a real implementation, this would use a specific energy function
        # For now, we'll use a simple quadratic function
        energy = 0.0
        
        for key, value in state.items():
            if isinstance(value, (int, float)):
                energy += value**2
            elif isinstance(value, str) and value.isdigit():
                energy += float(value)**2
        
        return energy


class ProofMetrics:
    """
    Calculates proof metrics based on the Chernoff Risk Bound to assess
    residual bug probability and ensure SLA compliance.
    """
    
    def __init__(self, storage_path: str = None):
        self.total_obligations = 0
        self.verified_obligations = 0
        self.failed_obligations = 0
        self.storage_path = storage_path
        self.historical_data = self._load_historical_data() if storage_path else {}
        
    def update_counts(self, total: int, verified: int, failed: int = 0) -> None:
        """
        Update obligation counts with verification results.
        
        Args:
            total: Total number of obligations
            verified: Number of verified obligations
            failed: Number of failed verifications
        """
        self.total_obligations = total
        self.verified_obligations = verified
        self.failed_obligations = failed
        
        # Update historical data
        if self.storage_path:
            self._update_historical_data()
        
    def calculate_residual_risk(self, f: float, n: int, rho: float, eps: float) -> float:
        """
        Calculate residual bug probability using Chernoff bound.
        
        Args:
            f: Fraction of obligations checked
            n: Number of samples
            rho: Historical base rate
            eps: Tolerated empirical failure rate
            
        Returns:
            Residual bug probability
        """
        # Calculate Chernoff bound term
        chernoff = 2 * math.exp(-2 * n * eps**2)
        
        # Calculate mixture probability
        # P_bug ≤ (1−f)ρ + f⋅2e^(−2nε²)
        p_bug = (1 - f) * rho + f * chernoff
        
        return p_bug
    
    def get_metrics(self) -> Tuple[float, int, float, float]:
        """
        Get proof metrics needed for the Chernoff bound.
        
        Returns:
            Tuple of (f, n, rho, eps)
                f: fraction of obligations checked
                n: number of samples
                rho: historical base rate
                eps: tolerated empirical failure rate
        """
        if self.total_obligations == 0:
            return 0.0, 0, 0.0, 0.0
            
        # Calculate fraction of obligations checked
        f = self.verified_obligations / self.total_obligations
        
        # Number of samples
        n = self.verified_obligations
        
        # Calculate historical base rate from stored data if available
        if self.historical_data and "total_checks" in self.historical_data and self.historical_data["total_checks"] > 1000:
            # Use historical failure rate if we have enough data
            rho = self.historical_data.get("failure_rate", 1e-4)
        else:
            # Default historical base rate (1 in 10,000)
            rho = 1e-4
            
        # Calculate empirical error rate from current verification results
        if self.verified_obligations > 0:
            current_err_rate = self.failed_obligations / self.verified_obligations
            # Blend historical and current rates for more stability
            rho = 0.9 * rho + 0.1 * current_err_rate
        
        # Tolerated empirical failure rate based on SLA requirements
        # In practice, this could be configured based on risk tolerance
        eps = 0.01  # Default: 1% tolerated error rate
        
        return f, n, rho, eps
    
    def _load_historical_data(self) -> Dict:
        """Load historical verification data"""
        try:
            if not self.storage_path:
                return {}
                
            import os
            history_file = os.path.join(self.storage_path, "verification_history.json")
            
            if os.path.exists(history_file):
                with open(history_file, 'r') as f:
                    return json.load(f)
            return {
                "total_checks": 0,
                "total_failures": 0,
                "failure_rate": 1e-4,
                "verification_runs": []
            }
        except Exception as e:
            logger.error(f"Failed to load historical verification data: {e}")
            return {}
    
    def _update_historical_data(self) -> None:
        """Update historical verification data with current run"""
        try:
            if not self.storage_path:
                return
                
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            history_file = os.path.join(self.storage_path, "verification_history.json")
            
            # Initialize if not loaded
            if not self.historical_data:
                self.historical_data = {
                    "total_checks": 0,
                    "total_failures": 0,
                    "failure_rate": 1e-4,
                    "verification_runs": []
                }
            
            # Add current run
            self.historical_data["total_checks"] += self.verified_obligations
            self.historical_data["total_failures"] += self.failed_obligations
            
            # Calculate updated failure rate
            if self.historical_data["total_checks"] > 0:
                self.historical_data["failure_rate"] = self.historical_data["total_failures"] / self.historical_data["total_checks"]
            
            # Add run data
            timestamp = datetime.datetime.now().isoformat()
            self.historical_data["verification_runs"].append({
                "timestamp": timestamp,
                "total": self.total_obligations,
                "verified": self.verified_obligations,
                "failed": self.failed_obligations,
                "coverage": self.verified_obligations / self.total_obligations if self.total_obligations > 0 else 0
            })
            
            # Keep only last 100 runs
            if len(self.historical_data["verification_runs"]) > 100:
                self.historical_data["verification_runs"] = self.historical_data["verification_runs"][-100:]
            
            # Save to file
            with open(history_file, 'w') as f:
                json.dump(self.historical_data, f, indent=2)
                
            logger.info(f"Updated verification history: {self.verified_obligations} checks, {self.failed_obligations} failures")
        except Exception as e:
            logger.error(f"Failed to update historical verification data: {e}")


class MetaAwareness:
    """
    Monitors meta-awareness guards to ensure system stability.
    
    Implements three key stability metrics:
    1. Lipschitz semantic drift: Measures how much embeddings drift over time
    2. Lindblad perturbation: Quantifies quantum noise in the system state
    3. Lyapunov trend: Ensures energy function is strictly decreasing
    """
    
    def __init__(self, storage_path: str = None):
        self.phi_values = []  # Store last 50 Φ values for trend analysis
        self.storage_path = storage_path
        self.state_history = []  # Track state evolution for drift calculation
        self.reference_embeddings = {}  # Reference embeddings for drift calculation
        self.perturbation_history = []  # Track perturbation values
        self._load_reference_embeddings()
    
    def _load_reference_embeddings(self) -> None:
        """Load reference embeddings from storage"""
        if not self.storage_path:
            # Initialize with some default reference embeddings
            # In a real system, these would be loaded from a file
            self.reference_embeddings = {
                "authenticate_user": [0.1, 0.2, 0.3, 0.4],
                "authorize_user": [0.2, 0.3, 0.4, 0.5],
                "validate_data": [0.3, 0.4, 0.5, 0.6]
            }
            return
            
        try:
            import os
            import numpy as np
            
            embeddings_file = os.path.join(self.storage_path, "reference_embeddings.npz")
            if os.path.exists(embeddings_file):
                data = np.load(embeddings_file, allow_pickle=True)
                self.reference_embeddings = data["embeddings"].item()
                logger.info(f"Loaded {len(self.reference_embeddings)} reference embeddings")
            else:
                # Initialize with defaults
                self.reference_embeddings = {
                    "authenticate_user": [0.1, 0.2, 0.3, 0.4],
                    "authorize_user": [0.2, 0.3, 0.4, 0.5],
                    "validate_data": [0.3, 0.4, 0.5, 0.6]
                }
        except Exception as e:
            logger.error(f"Failed to load reference embeddings: {e}")
            # Fall back to default embeddings
            self.reference_embeddings = {
                "authenticate_user": [0.1, 0.2, 0.3, 0.4],
                "authorize_user": [0.2, 0.3, 0.4, 0.5],
                "validate_data": [0.3, 0.4, 0.5, 0.6]
            }
    
    def update_state(self, state: Dict) -> None:
        """
        Update the current system state.
        
        Args:
            state: Current system state
        """
        # Add to state history
        self.state_history.append(state)
        
        # Keep only the last 100 states
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]
        
        # Calculate and add Φ value
        phi = self._calculate_phi(state)
        self.add_phi_value(phi)
        
        # Calculate and store perturbation
        perturb = self._calculate_perturbation(state)
        self.perturbation_history.append(perturb)
        
        # Keep only the last 100 perturbations
        if len(self.perturbation_history) > 100:
            self.perturbation_history = self.perturbation_history[-100:]
    
    def _calculate_phi(self, state: Dict) -> float:
        """
        Calculate Φ value for Lyapunov function.
        
        Args:
            state: Current system state
            
        Returns:
            Φ value
        """
        # In a real system, this would be a specific energy function
        # For demonstration, we use a simple function of state values
        phi = 0.0
        
        for key, value in state.items():
            if isinstance(value, (int, float)):
                phi += value**2
            elif isinstance(value, str) and value.isdigit():
                phi += float(value)**2
        
        return phi
    
    def _calculate_perturbation(self, state: Dict) -> float:
        """
        Calculate perturbation for current state.
        
        Args:
            state: Current system state
            
        Returns:
            Perturbation value
        """
        # In a real system, this would calculate quantum noise
        # For demonstration, we use a simple function based on state entropy
        
        # Calculate state entropy as a proxy for perturbation
        entropy = 0.0
        total = 0.0
        
        for key, value in state.items():
            if isinstance(value, (int, float)) and value > 0:
                entropy -= value * math.log(value)
                total += value
            elif isinstance(value, str) and value.isdigit():
                val = float(value)
                if val > 0:
                    entropy -= val * math.log(val)
                    total += val
        
        if total > 0:
            return entropy / total
        return 0.0
    
    def semantic_drift(self) -> float:
        """
        Calculate Lipschitz semantic drift by comparing current embeddings
        with reference embeddings. Uses L2 norm to compute distance.
        
        Returns:
            Drift value, should be ≤ 0.02
        """
        if not self.state_history:
            return 0.0  # No history, no drift
            
        # Get current state
        current_state = self.state_history[-1]
        
        # Calculate current embeddings
        # In a real system, this would use a proper embedding model
        current_embeddings = {}
        for key, value in current_state.items():
            if key in self.reference_embeddings:
                # Simulate embedding calculation
                emb = self.reference_embeddings[key].copy()
                
                # Add some variation based on current state
                if isinstance(value, (int, float)):
                    for i in range(len(emb)):
                        emb[i] += value * 0.001 * (i + 1)
                
                current_embeddings[key] = emb
        
        # Calculate maximum drift
        max_drift = 0.0
        for key, ref_emb in self.reference_embeddings.items():
            if key in current_embeddings:
                curr_emb = current_embeddings[key]
                
                # Calculate L2 distance
                dist = sum((ref_emb[i] - curr_emb[i])**2 for i in range(len(ref_emb))) ** 0.5
                
                # Update max drift
                max_drift = max(max_drift, dist)
        
        return max_drift
    
    def lindblad_perturb(self) -> float:
        """
        Calculate Lindblad perturbation based on trace distance
        between consecutive density matrices (represented by states).
        
        Returns:
            Perturbation value, should be ≤ ε/2
        """
        if len(self.perturbation_history) < 1:
            return 0.0  # No history, no perturbation
        
        # Return average of recent perturbations
        recent = self.perturbation_history[-5:] if len(self.perturbation_history) >= 5 else self.perturbation_history
        return sum(recent) / len(recent)
    
    def add_phi_value(self, phi: float) -> None:
        """
        Add a new Φ value to the history.
        
        Args:
            phi: New Φ value
        """
        self.phi_values.append(phi)
        # Keep only the last 50 values
        if len(self.phi_values) > 50:
            self.phi_values = self.phi_values[-50:]
    
    def lyapunov_monotone(self) -> bool:
        """
        Check if Lyapunov function is strictly decreasing over last 50 iterations.
        Implements a robust check that allows for small numerical fluctuations.
        
        Returns:
            True if monotonically decreasing, False otherwise
        """
        if len(self.phi_values) < 5:
            return True  # Not enough data to determine trend
        
        # Calculate moving average to smooth out small fluctuations
        window_size = 3
        smoothed = []
        for i in range(len(self.phi_values) - window_size + 1):
            window = self.phi_values[i:i+window_size]
            smoothed.append(sum(window) / window_size)
        
        # Check if smoothed values are decreasing
        # Allow for a small tolerance to account for numerical issues
        tolerance = 1e-10
        for i in range(1, len(smoothed)):
            if smoothed[i] > smoothed[i-1] + tolerance:
                return False
        
        return True
    
    def save_state(self) -> None:
        """Save current state to storage"""
        if not self.storage_path:
            return
            
        try:
            import os
            import numpy as np
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Save reference embeddings
            embeddings_file = os.path.join(self.storage_path, "reference_embeddings.npz")
            np.savez(embeddings_file, embeddings=self.reference_embeddings)
            
            # Save Φ values
            phi_file = os.path.join(self.storage_path, "phi_values.npz")
            np.savez(phi_file, phi=self.phi_values)
            
            logger.info("Saved MetaAwareness state")
        except Exception as e:
            logger.error(f"Failed to save MetaAwareness state: {e}")


class ErrorReporting:
    """
    Handles error reporting, categorization, and storage.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        self.error_categories = {
            "MISSING_OBLIGATION": "COMPLETENESS",
            "ENERGY_NOT_MINIMUM": "CORRECTNESS",
            "RISK_EXCEEDS_SLA": "CORRECTNESS",
            "LIP_DRIFT": "META_AWARENESS",
            "REFLECTION_UNSTABLE": "META_AWARENESS",
            "PHI_STAGNATION": "META_AWARENESS"
        }
        
    def generate_error_id(self) -> str:
        """Generate a unique error ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"ERR-{timestamp}-{unique}"
    
    def record_error(self, reason: str, details: Optional[Dict] = None) -> Dict:
        """
        Record an error with full context.
        
        Args:
            reason: The error reason/code
            details: Additional error details
            
        Returns:
            The complete error record
        """
        error_id = self.generate_error_id()
        timestamp = datetime.datetime.now().isoformat()
        category = self.error_categories.get(reason, "SYSTEM")
        
        error = {
            "error_id": error_id,
            "timestamp": timestamp,
            "category": category,
            "subcategory": reason,
            "severity": self._determine_severity(reason, details),
            "metrics": details,
            "affected_components": self._identify_affected_components(reason, details),
            "description": self._generate_description(reason, details),
            "recommended_actions": self._generate_recommendations(reason, details),
            "historical_context": self._get_historical_context(reason)
        }
        
        # Store the error
        self._store_error(error)
        
        logger.warning(f"Error recorded: {error_id} - {reason}")
        return error
    
    def _determine_severity(self, reason: str, details: Optional[Dict]) -> str:
        """Determine the severity of an error based on reason and details"""
        # Placeholder implementation
        severity_map = {
            "MISSING_OBLIGATION": "CRITICAL",
            "ENERGY_NOT_MINIMUM": "CRITICAL",
            "RISK_EXCEEDS_SLA": "HIGH",
            "LIP_DRIFT": "MEDIUM",
            "REFLECTION_UNSTABLE": "HIGH",
            "PHI_STAGNATION": "MEDIUM"
        }
        return severity_map.get(reason, "MEDIUM")
    
    def _identify_affected_components(self, reason: str, details: Optional[Dict]) -> List[str]:
        """Identify components affected by an error"""
        # Placeholder implementation
        return ["resource_manager.py", "load_balancer.py"]
    
    def _generate_description(self, reason: str, details: Optional[Dict]) -> str:
        """Generate a human-readable description of the error"""
        descriptions = {
            "MISSING_OBLIGATION": "Required obligation(s) not implemented in codebase",
            "ENERGY_NOT_MINIMUM": "Energy metric not converged to minimum; lambda exceeds threshold",
            "RISK_EXCEEDS_SLA": "Residual bug probability exceeds SLA threshold",
            "LIP_DRIFT": "Semantic drift exceeds Lipschitz bound",
            "REFLECTION_UNSTABLE": "Reflection perturbation exceeds stability threshold",
            "PHI_STAGNATION": "Lyapunov function not monotonically decreasing"
        }
        return descriptions.get(reason, f"Unknown error: {reason}")
    
    def _generate_recommendations(self, reason: str, details: Optional[Dict]) -> List[str]:
        """Generate recommended actions based on error reason"""
        # Placeholder implementation
        recommendations = {
            "MISSING_OBLIGATION": ["Implement missing obligations", "Verify obligation mapping"],
            "ENERGY_NOT_MINIMUM": ["Re-tune annealer parameters", "Check gradient calculation"],
            "RISK_EXCEEDS_SLA": ["Increase proof coverage", "Reduce empirical failure rate"],
            "LIP_DRIFT": ["Stabilize semantic embeddings", "Reduce learning rate"],
            "REFLECTION_UNSTABLE": ["Reduce perturbation amplitude", "Increase damping factor"],
            "PHI_STAGNATION": ["Check for regressions", "Verify test coverage"]
        }
        return recommendations.get(reason, ["Investigate error cause"])
    
    def _get_historical_context(self, reason: str) -> Dict:
        """
        Get historical context for an error type by analyzing previously recorded errors.
        
        Args:
            reason: The error reason/code
            
        Returns:
            Dictionary with historical context information
        """
        try:
            import os
            import glob
            import yaml
            from collections import Counter
            
            # Check if storage path exists
            if not os.path.exists(self.storage_path):
                return {
                    "related_errors": [],
                    "frequency": "First occurrence"
                }
            
            # Find all error files
            error_files = glob.glob(f"{self.storage_path}/ERR-*.yaml")
            
            if not error_files:
                return {
                    "related_errors": [],
                    "frequency": "First occurrence"
                }
            
            # Load all errors
            errors = []
            error_reasons = Counter()
            for file_path in error_files:
                try:
                    with open(file_path, 'r') as f:
                        error = yaml.safe_load(f)
                        if error and "subcategory" in error:
                            errors.append(error)
                            error_reasons[error["subcategory"]] += 1
                except Exception as e:
                    logger.warning(f"Failed to load error file {file_path}: {e}")
            
            # Find related errors (same subcategory)
            related_errors = []
            for error in errors:
                if error.get("subcategory") == reason and "error_id" in error:
                    related_errors.append(error["error_id"])
            
            # Determine frequency
            current_count = error_reasons[reason]
            total_errors = sum(error_reasons.values())
            
            if current_count == 0:
                frequency = "First occurrence"
            elif current_count == 1:
                frequency = "Second occurrence"
            elif current_count < 5:
                frequency = f"{current_count + 1}rd occurrence"
            else:
                percentage = (current_count / total_errors) * 100 if total_errors > 0 else 0
                frequency = f"Common error ({current_count} occurrences, {percentage:.1f}% of all errors)"
            
            # Find related components that often appear with this error
            related_components = Counter()
            for error in errors:
                if error.get("subcategory") == reason and "affected_components" in error:
                    for component in error["affected_components"]:
                        related_components[component] += 1
            
            # Get top related components
            top_components = [component for component, _ in related_components.most_common(3)]
            
            # Calculate time since last occurrence
            if related_errors:
                # Find most recent related error
                latest_error = None
                latest_timestamp = None
                for error in errors:
                    if error.get("subcategory") == reason and "timestamp" in error:
                        if latest_timestamp is None or error["timestamp"] > latest_timestamp:
                            latest_timestamp = error["timestamp"]
                            latest_error = error
                
                if latest_timestamp:
                    try:
                        import datetime
                        last_time = datetime.datetime.fromisoformat(latest_timestamp)
                        now = datetime.datetime.now()
                        time_diff = now - last_time
                        days = time_diff.days
                        hours = time_diff.seconds // 3600
                        
                        if days > 0:
                            time_since = f"{days} days ago"
                        else:
                            time_since = f"{hours} hours ago"
                    except Exception:
                        time_since = "Recently"
                else:
                    time_since = "Unknown"
            else:
                time_since = "N/A"
            
            return {
                "related_errors": related_errors[-5:],  # Last 5 related errors
                "frequency": frequency,
                "common_components": top_components,
                "last_occurrence": time_since
            }
        except Exception as e:
            logger.error(f"Failed to get historical context: {e}")
            return {
                "related_errors": [],
                "frequency": "Error retrieving history"
            }
    
    def _store_error(self, error: Dict) -> None:
        """Store error record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write error to file
            filename = f"{self.storage_path}/{error['error_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(error, f)
                
            logger.info(f"Error stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store error: {e}")


class FunctionalityGapRegistry:
    """
    Manages the registry of functionality gaps with proposed fixes.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    def generate_gap_id(self) -> str:
        """Generate a unique gap ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"GAP-{timestamp}-{unique}"
    
    def register_gap(self, component: str, description: str, impact: Dict) -> Dict:
        """
        Register a new functionality gap.
        
        Args:
            component: The affected component
            description: Description of the gap
            impact: Dictionary describing the impact
            
        Returns:
            The complete gap record
        """
        gap_id = self.generate_gap_id()
        gap = {
            "gap_id": gap_id,
            "discovery_date": datetime.datetime.now().strftime("%Y-%m-%d"),
            "status": "OPEN",
            "component": component,
            "description": description,
            "impact": impact,
            "proposed_fixes": [],
            "validation_criteria": []
        }
        
        # Store the gap
        self._store_gap(gap)
        
        logger.info(f"Functionality gap registered: {gap_id}")
        return gap
    
    def add_proposed_fix(self, gap_id: str, description: str, complexity: str, effort: str) -> Dict:
        """
        Add a proposed fix to a functionality gap.
        
        Args:
            gap_id: The gap ID
            description: Description of the fix
            complexity: Complexity level (LOW, MEDIUM, HIGH)
            effort: Estimated effort (e.g., "2 developer-days")
            
        Returns:
            The updated gap record
        """
        # Load the gap
        gap = self._load_gap(gap_id)
        if not gap:
            logger.error(f"Gap not found: {gap_id}")
            return {}
        
        # Generate fix ID
        fix_id = f"FIX-{len(gap['proposed_fixes']) + 1:03d}"
        
        # Add the fix
        fix = {
            "id": fix_id,
            "description": description,
            "complexity": complexity,
            "estimated_effort": effort,
            "status": "PROPOSED"
        }
        
        gap["proposed_fixes"].append(fix)
        
        # Store the updated gap
        self._store_gap(gap)
        
        logger.info(f"Proposed fix added to gap {gap_id}: {fix_id}")
        return gap
    
    def add_validation_criteria(self, gap_id: str, criteria: List[str]) -> Dict:
        """
        Add validation criteria to a functionality gap.
        
        Args:
            gap_id: The gap ID
            criteria: List of validation criteria
            
        Returns:
            The updated gap record
        """
        # Load the gap
        gap = self._load_gap(gap_id)
        if not gap:
            logger.error(f"Gap not found: {gap_id}")
            return {}
        
        # Add the criteria
        gap["validation_criteria"].extend(criteria)
        
        # Store the updated gap
        self._store_gap(gap)
        
        logger.info(f"Validation criteria added to gap {gap_id}")
        return gap
    
    def update_gap_status(self, gap_id: str, status: str) -> Dict:
        """
        Update the status of a functionality gap.
        
        Args:
            gap_id: The gap ID
            status: New status (OPEN, IN_PROGRESS, RESOLVED)
            
        Returns:
            The updated gap record
        """
        # Load the gap
        gap = self._load_gap(gap_id)
        if not gap:
            logger.error(f"Gap not found: {gap_id}")
            return {}
        
        # Update the status
        gap["status"] = status
        
        # Store the updated gap
        self._store_gap(gap)
        
        logger.info(f"Gap {gap_id} status updated to {status}")
        return gap
    
    def analyze_functionality_gaps(self) -> List[Dict]:
        """
        Analyze the system for functionality gaps by comparing implementations
        against requirements, performance benchmarks, and best practices.
        
        Returns:
            List of identified gaps
        """
        try:
            import os
            import json
            import yaml
            import re
            
            gaps = []
            
            # 1. Check for configuration-defined requirements
            requirements_path = os.path.join(os.path.dirname(self.storage_path), "requirements.json")
            if os.path.exists(requirements_path):
                # Load requirements
                try:
                    with open(requirements_path, 'r') as f:
                        requirements = json.load(f)
                        
                    # Check each requirement
                    for req in requirements.get("functional_requirements", []):
                        req_id = req.get("id")
                        req_name = req.get("name")
                        req_component = req.get("component")
                        req_description = req.get("description")
                        req_priority = req.get("priority", "MEDIUM")
                        
                        # Check if implementation exists for this requirement
                        impl_path = os.path.join(os.path.dirname(self.storage_path), "implementations")
                        if os.path.exists(impl_path):
                            # Look for implementation files
                            impl_files = os.listdir(impl_path)
                            impl_found = False
                            
                            for file in impl_files:
                                if re.search(f"{req_id}", file, re.IGNORECASE) or \
                                   re.search(re.escape(req_name), file, re.IGNORECASE):
                                    impl_found = True
                                    break
                            
                            if not impl_found:
                                # Gap found - requirement not implemented
                                gaps.append({
                                    "component": req_component,
                                    "description": f"Required functionality '{req_name}' not implemented",
                                    "impact": {
                                        "severity": "HIGH" if req_priority == "HIGH" else "MEDIUM",
                                        "affected_systems": [req_component],
                                        "performance_delta": "N/A"
                                    }
                                })
                except Exception as e:
                    logger.warning(f"Failed to load requirements: {e}")
            
            # 2. Check for performance gaps using benchmark data
            benchmarks_path = os.path.join(os.path.dirname(self.storage_path), "benchmarks")
            if os.path.exists(benchmarks_path):
                try:
                    # Find all benchmark files
                    benchmark_files = [f for f in os.listdir(benchmarks_path) 
                                     if f.startswith("BENCH-") and f.endswith(".yaml")]
                    
                    # Group benchmarks by target
                    benchmarks_by_target = {}
                    for file in benchmark_files:
                        try:
                            with open(os.path.join(benchmarks_path, file), 'r') as f:
                                benchmark = yaml.safe_load(f)
                                
                            target = benchmark.get("target")
                            if target:
                                if target not in benchmarks_by_target:
                                    benchmarks_by_target[target] = []
                                benchmarks_by_target[target].append(benchmark)
                        except Exception as e:
                            logger.warning(f"Failed to load benchmark file {file}: {e}")
                    
                    # Analyze each target's benchmarks for regressions
                    for target, target_benchmarks in benchmarks_by_target.items():
                        # Sort by timestamp
                        target_benchmarks.sort(key=lambda b: b.get("timestamp", ""))
                        
                        # Check for regressions by comparing the latest benchmark with previous ones
                        if len(target_benchmarks) >= 2:
                            latest = target_benchmarks[-1]
                            previous = target_benchmarks[-2]
                            
                            # Check each metric
                            for latest_metric in latest.get("metrics", []):
                                metric_name = latest_metric.get("name")
                                
                                # Find corresponding metric in previous benchmark
                                for prev_metric in previous.get("metrics", []):
                                    if prev_metric.get("name") == metric_name:
                                        # Check for regression
                                        latest_value = latest_metric.get("current")
                                        prev_value = prev_metric.get("current")
                                        
                                        if latest_value is not None and prev_value is not None:
                                            # Determine if this is a regression
                                            # For some metrics like latency, higher is worse
                                            # For others like throughput, lower is worse
                                            is_regression = False
                                            regression_pct = 0
                                            
                                            if metric_name in ["latency", "latency_p95", "response_time"]:
                                                # Higher is worse
                                                if latest_value > prev_value:
                                                    is_regression = True
                                                    regression_pct = ((latest_value - prev_value) / prev_value) * 100
                                            else:
                                                # Lower is worse (default assumption for throughput, etc.)
                                                if latest_value < prev_value:
                                                    is_regression = True
                                                    regression_pct = ((prev_value - latest_value) / prev_value) * 100
                                            
                                            if is_regression and regression_pct > 5:  # 5% threshold
                                                # Performance gap found
                                                gaps.append({
                                                    "component": target,
                                                    "description": f"Performance regression in {metric_name}",
                                                    "impact": {
                                                        "severity": "HIGH" if regression_pct > 20 else "MEDIUM",
                                                        "affected_systems": [target],
                                                        "performance_delta": f"-{regression_pct:.1f}%"
                                                    }
                                                })
                except Exception as e:
                    logger.warning(f"Failed to analyze benchmarks: {e}")
            
            # 3. Check for implementation gaps using static analysis
            # This would involve checking actual code against best practices,
            # code coverage, test coverage, etc.
            # In a production implementation, this would be more comprehensive
            
            return gaps
        except Exception as e:
            logger.error(f"Failed to analyze functionality gaps: {e}")
            return []
    
    def _store_gap(self, gap: Dict) -> None:
        """Store gap record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write gap to file
            filename = f"{self.storage_path}/{gap['gap_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(gap, f)
                
            logger.info(f"Gap stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store gap: {e}")
    
    def _load_gap(self, gap_id: str) -> Optional[Dict]:
        """Load gap record from data store"""
        try:
            filename = f"{self.storage_path}/{gap_id}.yaml"
            with open(filename, 'r') as f:
                gap = yaml.safe_load(f)
                
            return gap
        except Exception as e:
            logger.error(f"Failed to load gap {gap_id}: {e}")
            return None


class PatchManager:
    """
    Manages patches for both internal and external systems.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    def generate_internal_patch_id(self) -> str:
        """Generate a unique internal patch ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"INT-{timestamp}-{unique}"
    
    def generate_external_patch_id(self) -> str:
        """Generate a unique external patch ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"EXT-{timestamp}-{unique}"
    
    def record_internal_patch(self, component: str, patch_type: str, description: str, 
                             changes: List[Dict], metrics_impact: Dict, verification: Dict) -> Dict:
        """
        Record an internal patch.
        
        Args:
            component: The affected component
            patch_type: Type of patch (ENHANCEMENT, BUGFIX, SECURITY, PERFORMANCE)
            description: Description of the patch
            changes: List of file changes
            metrics_impact: Impact on metrics
            verification: Verification details
            
        Returns:
            The complete patch record
        """
        patch_id = self.generate_internal_patch_id()
        patch = {
            "patch_id": patch_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "component": component,
            "type": patch_type,
            "description": description,
            "changes": changes,
            "metrics_impact": metrics_impact,
            "verification": verification
        }
        
        # Store the patch
        self._store_patch(patch)
        
        logger.info(f"Internal patch recorded: {patch_id}")
        return patch
    
    def record_external_patch(self, target_system: str, affected_files: List[str], 
                             issue_type: str, description: str, root_cause: str, 
                             solution: str, before_metrics: Dict, after_metrics: Dict) -> Dict:
        """
        Record an external patch.
        
        Args:
            target_system: The target external system
            affected_files: List of affected files
            issue_type: Type of issue (BUGFIX, OPTIMIZATION, SECURITY)
            description: Description of the issue
            root_cause: Root cause of the issue
            solution: Solution implemented
            before_metrics: Metrics before the patch
            after_metrics: Metrics after the patch
            
        Returns:
            The complete patch record
        """
        patch_id = self.generate_external_patch_id()
        patch = {
            "patch_id": patch_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "target_system": target_system,
            "affected_files": affected_files,
            "issue_type": issue_type,
            "description": description,
            "root_cause": root_cause,
            "solution": solution,
            "before_metrics": before_metrics,
            "after_metrics": after_metrics,
            "customer_notification": {
                "status": "PENDING",
                "timestamp": None
            },
            "follow_up_required": False
        }
        
        # Store the patch
        self._store_patch(patch)
        
        logger.info(f"External patch recorded: {patch_id}")
        return patch
    
    def update_customer_notification(self, patch_id: str, status: str) -> Dict:
        """
        Update customer notification status for an external patch.
        
        Args:
            patch_id: The patch ID
            status: Notification status (PENDING, SENT, FAILED)
            
        Returns:
            The updated patch record
        """
        # Load the patch
        patch = self._load_patch(patch_id)
        if not patch:
            logger.error(f"Patch not found: {patch_id}")
            return {}
        
        # Update the notification status
        patch["customer_notification"]["status"] = status
        patch["customer_notification"]["timestamp"] = datetime.datetime.now().isoformat()
        
        # Store the updated patch
        self._store_patch(patch)
        
        logger.info(f"Customer notification for patch {patch_id} updated to {status}")
        return patch
    
    def get_patches_by_component(self, component: str) -> List[Dict]:
        """
        Get all patches for a specific component.
        
        Args:
            component: The component name
            
        Returns:
            List of patches for the component
        """
        # Placeholder implementation
        # In a real system, this would query the data store
        return []
    
    def _store_patch(self, patch: Dict) -> None:
        """Store patch record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write patch to file
            filename = f"{self.storage_path}/{patch['patch_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(patch, f)
                
            logger.info(f"Patch stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store patch: {e}")
    
    def _load_patch(self, patch_id: str) -> Optional[Dict]:
        """Load patch record from data store"""
        try:
            filename = f"{self.storage_path}/{patch_id}.yaml"
            with open(filename, 'r') as f:
                patch = yaml.safe_load(f)
                
            return patch
        except Exception as e:
            logger.error(f"Failed to load patch {patch_id}: {e}")
            return None


class BugTracker:
    """
    Tracks bugs throughout their lifecycle.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    def generate_bug_id(self) -> str:
        """Generate a unique bug ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"BUG-{timestamp}-{unique}"
    
    def record_bug(self, discovery_method: str, reporter: str, bug_type: str, 
                  severity: str, priority: str, reproducibility: str, 
                  affected_components: List[str], symptoms: str, root_cause: str = None) -> Dict:
        """
        Record a new bug.
        
        Args:
            discovery_method: How the bug was discovered
            reporter: Who/what reported the bug
            bug_type: Type of bug
            severity: Severity level
            priority: Priority level
            reproducibility: How reproducible the bug is
            affected_components: List of affected components
            symptoms: Description of symptoms
            root_cause: Root cause if known
            
        Returns:
            The complete bug record
        """
        bug_id = self.generate_bug_id()
        bug = {
            "bug_id": bug_id,
            "discovery": {
                "timestamp": datetime.datetime.now().isoformat(),
                "method": discovery_method,
                "reporter": reporter
            },
            "classification": {
                "type": bug_type,
                "severity": severity,
                "priority": priority,
                "reproducibility": reproducibility
            },
            "affected_components": affected_components,
            "symptoms": symptoms,
            "root_cause": root_cause,
            "fix": {
                "status": "OPEN",
                "resolution_time": None,
                "patch_id": None,
                "commit_hash": None,
                "changed_files": None,
                "lines_changed": None
            },
            "verification": {
                "test_case": None,
                "runs": None,
                "success_rate": None
            },
            "metrics": {
                "mttr": None,
                "defect_density": None,
                "regression_probability": None
            }
        }
        
        # Store the bug
        self._store_bug(bug)
        
        logger.info(f"Bug recorded: {bug_id}")
        return bug
    
    def update_bug_fix(self, bug_id: str, status: str, patch_id: str = None, 
                      commit_hash: str = None, changed_files: int = None, 
                      lines_changed: int = None) -> Dict:
        """
        Update the fix information for a bug.
        
        Args:
            bug_id: The bug ID
            status: Fix status (OPEN, IN_PROGRESS, RESOLVED)
            patch_id: Associated patch ID
            commit_hash: Commit hash of the fix
            changed_files: Number of files changed
            lines_changed: Number of lines changed
            
        Returns:
            The updated bug record
        """
        # Load the bug
        bug = self._load_bug(bug_id)
        if not bug:
            logger.error(f"Bug not found: {bug_id}")
            return {}
        
        # Update the fix information
        bug["fix"]["status"] = status
        
        if status == "RESOLVED":
            # Calculate resolution time
            start_time = datetime.datetime.fromisoformat(bug["discovery"]["timestamp"])
            end_time = datetime.datetime.now()
            resolution_time = end_time - start_time
            bug["fix"]["resolution_time"] = str(resolution_time)
            bug["metrics"]["mttr"] = str(resolution_time)
        
        if patch_id:
            bug["fix"]["patch_id"] = patch_id
        
        if commit_hash:
            bug["fix"]["commit_hash"] = commit_hash
        
        if changed_files is not None:
            bug["fix"]["changed_files"] = changed_files
        
        if lines_changed is not None:
            bug["fix"]["lines_changed"] = lines_changed
        
        # Store the updated bug
        self._store_bug(bug)
        
        logger.info(f"Bug {bug_id} fix updated to {status}")
        return bug
    
    def update_verification(self, bug_id: str, test_case: str, runs: int, success_rate: str) -> Dict:
        """
        Update the verification information for a bug.
        
        Args:
            bug_id: The bug ID
            test_case: Test case used for verification
            runs: Number of test runs
            success_rate: Success rate of the tests
            
        Returns:
            The updated bug record
        """
        # Load the bug
        bug = self._load_bug(bug_id)
        if not bug:
            logger.error(f"Bug not found: {bug_id}")
            return {}
        
        # Update the verification information
        bug["verification"]["test_case"] = test_case
        bug["verification"]["runs"] = runs
        bug["verification"]["success_rate"] = success_rate
        
        # Calculate regression probability based on success rate
        if success_rate.endswith("%"):
            success_pct = float(success_rate.rstrip("%")) / 100
            if success_pct == 1.0:
                bug["metrics"]["regression_probability"] = "< 0.001%"
            else:
                bug["metrics"]["regression_probability"] = f"{(1 - success_pct) * 100:.3f}%"
        
        # Store the updated bug
        self._store_bug(bug)
        
        logger.info(f"Bug {bug_id} verification updated")
        return bug
    
    def get_open_bugs(self) -> List[Dict]:
        """Get all open bugs"""
        # Placeholder implementation
        # In a real system, this would query the data store
        return []
    
    def get_bugs_by_component(self, component: str) -> List[Dict]:
        """Get all bugs for a specific component"""
        # Placeholder implementation
        # In a real system, this would query the data store
        return []
    
    def _store_bug(self, bug: Dict) -> None:
        """Store bug record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write bug to file
            filename = f"{self.storage_path}/{bug['bug_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(bug, f)
                
            logger.info(f"Bug stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store bug: {e}")
    
    def _load_bug(self, bug_id: str) -> Optional[Dict]:
        """Load bug record from data store"""
        try:
            filename = f"{self.storage_path}/{bug_id}.yaml"
            with open(filename, 'r') as f:
                bug = yaml.safe_load(f)
                
            return bug
        except Exception as e:
            logger.error(f"Failed to load bug {bug_id}: {e}")
            return None


class BenchmarkingSystem:
    """
    Manages system benchmarks and tracks performance metrics over time.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    def generate_benchmark_id(self) -> str:
        """Generate a unique benchmark ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"BENCH-{timestamp}-{unique}"
    
    def record_benchmark(self, benchmark_type: str, target: str, environment: Dict,
                        metrics: List[Dict], test_scenario: str, 
                        consistency: Dict, regression_analysis: Dict) -> Dict:
        """
        Record a benchmark.
        
        Args:
            benchmark_type: Type of benchmark (PERFORMANCE, RELIABILITY, SCALABILITY)
            target: The target component
            environment: Environment details
            metrics: List of metrics
            test_scenario: Description of the test scenario
            consistency: Consistency metrics
            regression_analysis: Regression analysis
            
        Returns:
            The complete benchmark record
        """
        benchmark_id = self.generate_benchmark_id()
        benchmark = {
            "benchmark_id": benchmark_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "type": benchmark_type,
            "target": target,
            "environment": environment,
            "metrics": metrics,
            "test_scenario": test_scenario,
            "consistency": consistency,
            "regression_analysis": regression_analysis
        }
        
        # Store the benchmark
        self._store_benchmark(benchmark)
        
        logger.info(f"Benchmark recorded: {benchmark_id}")
        return benchmark
    
    def run_benchmark(self, target: str, benchmark_config: Dict) -> Dict:
        """
        Run a benchmark for a specific target.
        
        Args:
            target: The target component
            benchmark_config: Benchmark configuration
            
        Returns:
            The benchmark results
        """
        # Placeholder implementation
        # In a real system, this would run the actual benchmark
        
        # Return dummy results
        return {
            "metrics": [
                {
                    "name": "throughput",
                    "unit": "requests/sec",
                    "baseline": 1240,
                    "current": 1450,
                    "change_pct": "+16.9%"
                },
                {
                    "name": "latency_p95",
                    "unit": "ms",
                    "baseline": 120,
                    "current": 85,
                    "change_pct": "-29.2%"
                }
            ],
            "consistency": {
                "runs": 10,
                "std_deviation_pct": 2.3
            }
        }
    
    def get_benchmarks_for_target(self, target: str) -> List[Dict]:
        """Get all benchmarks for a specific target"""
        # Placeholder implementation
        # In a real system, this would query the data store
        return []
    
    def get_benchmark_trends(self, target: str, metric_name: str, 
                            time_range: Tuple[datetime.datetime, datetime.datetime]) -> Dict:
        """Get trends for a specific metric over time"""
        # Placeholder implementation
        # In a real system, this would query the data store and compute trends
        return {
            "target": target,
            "metric": metric_name,
            "time_range": [t.isoformat() for t in time_range],
            "data_points": [],
            "trend": "IMPROVING"  # IMPROVING, STABLE, DEGRADING
        }
    
    def _store_benchmark(self, benchmark: Dict) -> None:
        """Store benchmark record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write benchmark to file
            filename = f"{self.storage_path}/{benchmark['benchmark_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(benchmark, f)
                
            logger.info(f"Benchmark stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store benchmark: {e}")
    
    def _load_benchmark(self, benchmark_id: str) -> Optional[Dict]:
        """Load benchmark record from data store"""
        try:
            filename = f"{self.storage_path}/{benchmark_id}.yaml"
            with open(filename, 'r') as f:
                benchmark = yaml.safe_load(f)
                
            return benchmark
        except Exception as e:
            logger.error(f"Failed to load benchmark {benchmark_id}: {e}")
            return None


class ArchitecturalEnhancementRegistry:
    """
    Manages the registry of architectural enhancements.
    """
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        
    def generate_enhancement_id(self) -> str:
        """Generate a unique enhancement ID"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d")
        unique = str(uuid.uuid4())[:8]
        return f"ARCH-{timestamp}-{unique}"
    
    def record_enhancement(self, title: str, enhancement_type: str, status: str,
                          components_affected: List[str], description: str,
                          motivation: List[str], design_principles: List[str],
                          metrics: Dict, lessons_learned: List[str] = None,
                          future_extensions: List[str] = None) -> Dict:
        """
        Record an architectural enhancement.
        
        Args:
            title: The enhancement title
            enhancement_type: Type of enhancement (ARCHITECTURAL, INFRASTRUCTURE, ALGORITHM)
            status: Status of the enhancement (PROPOSED, IN_PROGRESS, IMPLEMENTED)
            components_affected: List of affected components
            description: Description of the enhancement
            motivation: List of motivation points
            design_principles: List of design principles
            metrics: Metrics before and after
            lessons_learned: List of lessons learned
            future_extensions: List of future extensions
            
        Returns:
            The complete enhancement record
        """
        enhancement_id = self.generate_enhancement_id()
        enhancement = {
            "enhancement_id": enhancement_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "title": title,
            "type": enhancement_type,
            "status": status,
            "components_affected": components_affected,
            "description": description,
            "motivation": motivation,
            "design_principles": design_principles,
            "metrics": metrics,
            "lessons_learned": lessons_learned or [],
            "future_extensions": future_extensions or []
        }
        
        # Store the enhancement
        self._store_enhancement(enhancement)
        
        logger.info(f"Architectural enhancement recorded: {enhancement_id}")
        return enhancement
    
    def update_enhancement_status(self, enhancement_id: str, status: str) -> Dict:
        """
        Update the status of an architectural enhancement.
        
        Args:
            enhancement_id: The enhancement ID
            status: New status (PROPOSED, IN_PROGRESS, IMPLEMENTED)
            
        Returns:
            The updated enhancement record
        """
        # Load the enhancement
        enhancement = self._load_enhancement(enhancement_id)
        if not enhancement:
            logger.error(f"Enhancement not found: {enhancement_id}")
            return {}
        
        # Update the status
        enhancement["status"] = status
        
        # Store the updated enhancement
        self._store_enhancement(enhancement)
        
        logger.info(f"Enhancement {enhancement_id} status updated to {status}")
        return enhancement
    
    def get_enhancements_by_component(self, component: str) -> List[Dict]:
        """Get all enhancements for a specific component"""
        # Placeholder implementation
        # In a real system, this would query the data store
        return []
    
    def _store_enhancement(self, enhancement: Dict) -> None:
        """Store enhancement record in data store"""
        try:
            # Ensure directory exists
            import os
            os.makedirs(self.storage_path, exist_ok=True)
            
            # Write enhancement to file
            filename = f"{self.storage_path}/{enhancement['enhancement_id']}.yaml"
            with open(filename, 'w') as f:
                yaml.dump(enhancement, f)
                
            logger.info(f"Enhancement stored to {filename}")
        except Exception as e:
            logger.error(f"Failed to store enhancement: {e}")
    
    def _load_enhancement(self, enhancement_id: str) -> Optional[Dict]:
        """Load enhancement record from data store"""
        try:
            filename = f"{self.storage_path}/{enhancement_id}.yaml"
            with open(filename, 'r') as f:
                enhancement = yaml.safe_load(f)
                
            return enhancement
        except Exception as e:
            logger.error(f"Failed to load enhancement {enhancement_id}: {e}")
            return None


class Auditor:
    """
    Main Auditor class that performs the complete audit process.
    
    The Auditor certifies:
    1. Completeness (every Δ-derived obligation exists)
    2. Correctness (global energy at optimum & proofs ≥ 90% coverage)
    3. Meta-awareness (all watch-dog invariants intact)
    
    It emits the single authoritative Audit-Stamp that freezes the build or,
    if any invariant breaks, vetoes further merges with a minimal defect log.
    """
    
    def __init__(self, config: Dict):
        """
        Initialize the Auditor with the provided configuration.
        
        Args:
            config: Auditor configuration dictionary
        """
        self.config = config
        
        # Set up storage paths
        self.data_path = config.get("data_path", "auditor_data")
        
        # Initialize components
        self.obligation_ledger = ObligationLedger()
        self.repo_modules = RepoModules(config.get("repo_path", "."))
        self.energy_calculator = EnergyCalculator()
        self.proof_metrics = ProofMetrics()
        self.meta_awareness = MetaAwareness()
        
        # Initialize data storage components
        self.error_reporting = ErrorReporting(f"{self.data_path}/errors")
        self.gap_registry = FunctionalityGapRegistry(f"{self.data_path}/gaps")
        self.patch_manager = PatchManager(f"{self.data_path}/patches")
        self.bug_tracker = BugTracker(f"{self.data_path}/bugs")
        self.benchmarking = BenchmarkingSystem(f"{self.data_path}/benchmarks")
        self.enhancement_registry = ArchitecturalEnhancementRegistry(f"{self.data_path}/enhancements")
    
    def run_audit(self) -> Dict:
        """
        Run the complete audit process.
        
        Returns:
            The audit result with Audit-Stamp
        """
        logger.info("Starting audit process...")
        
        # 1. Completeness Check
        logger.info("Performing completeness check...")
        try:
            # Load delta rules
            self.obligation_ledger.load_delta_rules(self.config.get("delta_rules_file", "delta_rules.json"))
            
            # Extract initial goals
            initial_goals = self._extract_initial_goals()
            
            # Compute Δ-closure
            obligations = self.obligation_ledger.compute_delta_closure(initial_goals)
            
            # Scan repository
            modules = self.repo_modules.scan_repository()
            
            # Check for missing obligations
            missing = obligations - modules
            if missing:
                logger.warning(f"Found {len(missing)} missing obligations")
                self.error_reporting.record_error("MISSING_OBLIGATION", {"missing": list(missing)})
                return self._fail("MISSING_OBLIGATION", {"missing": list(missing)})
            
            logger.info("Completeness check passed")
        except Exception as e:
            logger.error(f"Completeness check failed: {e}")
            return self._fail("COMPLETENESS_CHECK_ERROR", {"error": str(e)})
        
        # 2. Correctness Check
        logger.info("Performing correctness check...")
        try:
            # Get energy metrics
            E, delta_E, lamb = self.energy_calculator.get_metrics()
            if not (delta_E < 1e-7 and lamb < 0.9):
                logger.warning(f"Energy not at minimum: E={E}, ΔE={delta_E}, λ={lamb}")
                self.error_reporting.record_error("ENERGY_NOT_MINIMUM", {"E": E, "ΔE": delta_E, "λ": lamb})
                return self._fail("ENERGY_NOT_MINIMUM", {"E": E, "ΔE": delta_E, "λ": lamb})
            
            # Get proof metrics
            f, n, rho, eps = self.proof_metrics.get_metrics()
            p_bug = (1 - f) * rho + f * 2 * math.exp(-2 * n * eps**2)
            if p_bug > 1.1e-4:
                logger.warning(f"Risk exceeds SLA: P_bug={p_bug}")
                self.error_reporting.record_error("RISK_EXCEEDS_SLA", {"P_bug": p_bug})
                return self._fail("RISK_EXCEEDS_SLA", {"P_bug": p_bug})
            
            logger.info("Correctness check passed")
        except Exception as e:
            logger.error(f"Correctness check failed: {e}")
            return self._fail("CORRECTNESS_CHECK_ERROR", {"error": str(e)})
        
        # 3. Meta-awareness Check
        logger.info("Performing meta-awareness check...")
        try:
            # Check Lipschitz drift
            drift = self.meta_awareness.semantic_drift()
            if drift > 0.02:
                logger.warning(f"Lipschitz drift exceeds bound: {drift}")
                self.error_reporting.record_error("LIP_DRIFT", {"drift": drift})
                return self._fail("LIP_DRIFT", {"drift": drift})
            
            # Check Lindblad perturbation
            perturb = self.meta_awareness.lindblad_perturb()
            if perturb > eps / 2:
                logger.warning(f"Reflection perturbation unstable: {perturb}")
                self.error_reporting.record_error("REFLECTION_UNSTABLE", {"perturb": perturb})
                return self._fail("REFLECTION_UNSTABLE", {"perturb": perturb})
            
            # Check Lyapunov trend
            if not self.meta_awareness.lyapunov_monotone():
                logger.warning("Lyapunov function not monotonically decreasing")
                self.error_reporting.record_error("PHI_STAGNATION")
                return self._fail("PHI_STAGNATION")
            
            logger.info("Meta-awareness check passed")
        except Exception as e:
            logger.error(f"Meta-awareness check failed: {e}")
            return self._fail("META_AWARENESS_CHECK_ERROR", {"error": str(e)})
        
        # 4. Store metrics and benchmarks
        logger.info("Storing metrics and updating benchmarks...")
        try:
            self._store_metrics(E, delta_E, lamb, f, n, rho, eps, p_bug)
            self._update_benchmarks()
        except Exception as e:
            logger.warning(f"Failed to store metrics: {e}")
            # Non-critical, continue
        
        # 5. Run gap analysis and update registry
        logger.info("Running gap analysis...")
        try:
            gaps = self.gap_registry.analyze_functionality_gaps()
            for gap in gaps:
                self.gap_registry.register_gap(
                    gap["component"],
                    gap["description"],
                    gap["impact"]
                )
        except Exception as e:
            logger.warning(f"Failed to run gap analysis: {e}")
            # Non-critical, continue
        
        # 6. All checks passed, emit Audit-Stamp
        logger.info("All checks passed, emitting Audit-Stamp")
        return self._pass_audit()
    
    def _extract_initial_goals(self) -> Set[str]:
        """Extract initial goals from user request"""
        # Placeholder implementation
        # In a real system, this would extract goals from user request
        return {"goal1", "goal2", "goal3"}
    
    def _store_metrics(self, E, delta_E, lamb, f, n, rho, eps, p_bug) -> None:
        """Store metrics in data store"""
        # Placeholder implementation
        pass
    
    def _update_benchmarks(self) -> None:
        """Run benchmarks and store results"""
        # Placeholder implementation
        pass
    
    def _fail(self, reason: str, details: Optional[Dict] = None) -> Dict:
        """
        Generate a FAIL Audit-Stamp.
        
        Args:
            reason: The reason for failure
            details: Additional details
            
        Returns:
            The Audit-Stamp
        """
        stamp = {
            "audit_stamp": {
                "status": "FAIL",
                "reason": reason,
                "details": details,
                "timestamp": datetime.datetime.now().isoformat()
            }
        }
        
        # Log the failure
        logger.warning(f"Audit failed: {reason}")
        
        return stamp
    
    def _pass_audit(self) -> Dict:
        """
        Generate a PASS Audit-Stamp.
        
        Returns:
            The Audit-Stamp
        """
        stamp = {
            "audit_stamp": {
                "status": "PASS",
                "timestamp": datetime.datetime.now().isoformat()
            }
        }
        
        # Log the success
        logger.info("Audit passed")
        
        return stamp


def run_auditor(config_file: str) -> Dict:
    """
    Run the Auditor with the specified configuration file.
    
    Args:
        config_file: Path to the auditor configuration file
        
    Returns:
        The audit result
    """
    try:
        # Load configuration
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Initialize and run auditor
        auditor = Auditor(config)
        result = auditor.run_audit()
        
        return result
    except Exception as e:
        logger.error(f"Failed to run auditor: {e}")
        return {
            "audit_stamp": {
                "status": "ERROR",
                "reason": "AUDITOR_EXECUTION_FAILED",
                "details": {"error": str(e)},
                "timestamp": datetime.datetime.now().isoformat()
            }
        }


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python auditor.py <config_file>")
        sys.exit(1)
    
    config_file = sys.argv[1]
    result = run_auditor(config_file)
    
    # Print audit result
    print(yaml.dump(result))
