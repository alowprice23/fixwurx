"""
learning/replay_buffer.py
─────────────────────────
Tiny cyclic **experience replay buffer** that keeps the last *N* bug-resolution
“episodes” so the `AdaptiveOptimizer` (or any other learner) can sample
i.i.d. mini-batches.

Why a replay buffer?
────────────────────
*  Stabilises gradient updates – prevents bias towards the most recent episode.
*  Enables **off-policy** learning: Optimiser can re-evaluate old experience
   when its internal target changes (e.g. new reward function).
*  Lightweight –  <80 LOC, pure std-lib.

Data model
──────────
Every episode is stored as a *dict* with the canonical keys produced by
`MetaAgent.maybe_update()` **plus** anything the caller wishes to include.
Typical example::

    {
      "bugs_seen": 314,
      "success_rate": 0.92,
      "mean_tokens": 1034,
      "reward": 1.0,                # optional
      "timestamp": 1_723_456_789
    }

Public API
──────────
    buf = ReplayBuffer(capacity=500)
    buf.add(episode_dict)
    batch = buf.sample(batch_size=32)

Unit tests can monkey-patch `random.sample` for determinism.
"""

from __future__ import annotations

import random
import time
from collections import deque
from typing import Deque, Dict, List

__all__ = ["ReplayBuffer"]


class ReplayBuffer:
    """
    Cyclic deque with O(1) append; uniform random sampling without replacement.
    """

    def __init__(self, capacity: int = 1000) -> None:
        if capacity <= 0:
            raise ValueError("capacity must be positive")
        self._buf: Deque[Dict] = deque(maxlen=capacity)
        self.capacity = capacity

    # --------------------------------------------------------------------- add
    def add(self, episode: Dict) -> None:
        """
        Append a new episode dictionary; adds a 'timestamp' if missing.
        """
        if "timestamp" not in episode:
            episode = {**episode, "timestamp": time.time()}
        self._buf.append(episode)

    # ------------------------------------------------------------------ sample
    def sample(self, batch_size: int) -> List[Dict]:
        """
        Uniformly sample up to *batch_size* distinct episodes.
        If the buffer currently holds fewer than *batch_size* items, returns all.
        """
        n = len(self._buf)
        if n == 0:
            return []
        k = min(batch_size, n)
        # deque → list for O(1) indexing; copy not large (<=capacity)
        return random.sample(list(self._buf), k)

    # -------------------------------------------------------------- utilities
    def __len__(self) -> int:  # noqa: Dunder
        return len(self._buf)

    def is_full(self) -> bool:
        """Return True when buffer reached `capacity`."""
        return len(self._buf) == self.capacity

    def clear(self) -> None:
        """Drop all stored episodes (mainly for unit tests)."""
        self._buf.clear()

    # -------------------------------------------------------------- repr/debug
    def __repr__(self) -> str:  # noqa: Dunder
        return f"<ReplayBuffer {len(self)}/{self.capacity}>"


# ---------------------------------------------------------------------------—
# Example usage (manual test)
# ---------------------------------------------------------------------------—
if __name__ == "__main__":  # pragma: no cover
    rb = ReplayBuffer(capacity=5)
    for i in range(7):  # add more than capacity to test cyclic behaviour
        rb.add({"bugs_seen": i, "success_rate": 0.8 + 0.01 * i})
    print(rb)
    print("Sample 3:", rb.sample(3))
